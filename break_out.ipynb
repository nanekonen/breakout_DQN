{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import Output\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity): # capacityサイズのFIFOを生成\n",
    "        self.memory = deque([], maxlen=capacity)#空の配列,capacityサイズでdequeを作成\n",
    "\n",
    "    def push(self, *args): # メモリにデータを入れる　\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size): # batch_size分ランダムにメモリから抽出\n",
    "        return random.sample(self.memory, batch_size)#sample(,)で重複なしの要素がランダムに抽出される\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_replay_memory = 100000 #メモリのサイズ\n",
    "memory = ReplayMemory(capacity=size_replay_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dueling_Network(nn.Module):\n",
    "    def __init__(self, n_frame, n_actions):\n",
    "        super(Dueling_Network, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(n_frame,32,8,4)\n",
    "        self.conv2 = nn.Conv2d(32,64,4,2)\n",
    "        self.conv3 = nn.Conv2d(64,64,3,1)\n",
    "        self.act_fc = nn.Linear(3136 , 512)\n",
    "        self.act_fc2 = nn.Linear(512, n_actions)\n",
    "        self.value_fc = nn.Linear(3136 , 512)\n",
    "        self.value_fc2 = nn.Linear(512, 1)\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv3.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.act_fc.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.act_fc2.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.value_fc.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.value_fc2.weight)      \n",
    "        self.flatten = nn.Flatten()  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x_act = self.relu(self.act_fc(x))\n",
    "        x_act = self.act_fc2(x_act)\n",
    "        x_val = self.relu(self.value_fc(x))\n",
    "        x_val = self.value_fc2(x_val)\n",
    "        x_act_ave = torch.mean(x_act, dim=1, keepdim=True)\n",
    "        q = x_val + x_act - x_act_ave\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy_select_action(state , total_steps):\n",
    "    global fire_ball\n",
    "\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * total_steps / EPS_DECAY) # ランダムアクション選択の閾値を更新\n",
    "    \n",
    "    if fire_ball: # ボールが落ちた場合は、強制的にボールを出す\n",
    "        fire_ball = False\n",
    "        return torch.tensor([[1]])\n",
    "    elif random.random() > eps_threshold:  # モデルによるアクション\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state.to(device)).argmax().view(1, 1).cpu()\n",
    "    else: # ランダムアクション\n",
    "        return torch.tensor([[env.action_space.sample()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boltzman(state,i_episode):\n",
    "    with torch.no_grad():\n",
    "        q = policy_net(state.to(device))\n",
    "        temperature = 1 / math.log(i_episode/BOLTZMAN_TEMPERATURE_DECAY + 1.1)\n",
    "        qexp = torch.exp(q / temperature )\n",
    "        qexpSum = qexp.sum()\n",
    "        ratio = qexp / qexpSum\n",
    "        return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boltzman_select_action(state,i_episode):\n",
    "    global fire_ball\n",
    "    \n",
    "    if fire_ball: # ボールが落ちた場合は、強制的にボールを出す\n",
    "        fire_ball = False\n",
    "        return torch.tensor([[1]])\n",
    "    else:\n",
    "        ratio = boltzman(state,i_episode)\n",
    "        ratioStep = 0\n",
    "        R = random.random()\n",
    "        for i in range(len(ratio[0])):\n",
    "            ratioStep += ratio[0,i]\n",
    "            if R <= ratioStep:  # モデルによるアクション\n",
    "                return torch.tensor([[i]])\n",
    "        return torch.tensor([[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_resize_gray(image, resize):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)[32:,8:152] # プレイに関係ある部分のみ切り出し\n",
    "    image = cv2.resize(src=image, dsize=(resize, resize))/255. # 画像のリサイズと値の変換0～1\n",
    "    for _ in range(10): # ブロックのあるエリアをランダムに削除\n",
    "        if random.random() > 0.9:\n",
    "            x_p = random.randint(10, 25)\n",
    "            y_p = random.randint(0, 70)\n",
    "            image[x_p:x_p+4, y_p:y_p+10] = 0.0\n",
    "    image = torch.tensor(image, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    \n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state).to(device)\n",
    "    action_batch = torch.cat(batch.action).to(device)\n",
    "    reward_batch = torch.cat(batch.reward).to(device)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states.to(device)).max(1)[0]\n",
    "    \n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 1000)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500000 # 学習させるエピソード数\n",
    "    \n",
    "n_frame = 6 # 過去フレームの利用数\n",
    "resize_image = 84 # リサイズ後のピクセル数\n",
    "BATCH_SIZE = 256 # バッチサイズ\n",
    "\n",
    "# ε-greedy方策パラメータ\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 5*num_episodes\n",
    "\n",
    "# boltzman方策パラメータ\n",
    "BOLTZMAN_TEMPERATURE_DECAY = num_episodes / 1000\n",
    "\n",
    "# モデル更新パラメータ\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "GAMMA = 0.99\n",
    "\n",
    "num_episode_save = 25000\n",
    "\n",
    "# モデルの初期化\n",
    "env = gym.make('ALE/Breakout-v5', render_mode=\"rgb_array\")\n",
    "n_actions = env.action_space.n\n",
    "policy_net = Dueling_Network(n_frame, n_actions).to(device)\n",
    "target_net = Dueling_Network(n_frame, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d2ee9c37cf4774adee2d251144332a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb13b41cd234b0984e0f9c97b9b85c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[0;32m     22\u001b[0m     total_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 23\u001b[0m     action \u001b[38;5;241m=\u001b[39m boltzman_select_action(state_frame, i_episode)\n\u001b[0;32m     24\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m old_life \u001b[38;5;241m>\u001b[39m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlives\u001b[39m\u001b[38;5;124m'\u001b[39m]: \u001b[38;5;66;03m# ライフが減った場合にtruncatedをTrueにして次エピソードにする\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[71], line 13\u001b[0m, in \u001b[0;36mboltzman_select_action\u001b[1;34m(state, i_episode)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ratio[\u001b[38;5;241m0\u001b[39m])):\n\u001b[0;32m     12\u001b[0m     ratioStep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ratio[\u001b[38;5;241m0\u001b[39m,i]\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m R \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m ratioStep:  \u001b[38;5;66;03m# モデルによるアクション\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([[i]])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([[i]])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "terminated = True\n",
    "frame1 = True\n",
    "total_steps = 0\n",
    "count_update = 0\n",
    "steps_done = 0\n",
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "for i_episode in tqdm(range(num_episodes)):\n",
    "    fire_ball = True # エピソードの初めは必ずボールを出すアクションにする\n",
    "    reward_frame = torch.tensor([0], dtype=torch.float32)  \n",
    "    if terminated == True:\n",
    "        state_frame = torch.zeros((1, n_frame, resize_image, resize_image), dtype=torch.float32)\n",
    "        next_state_frame = torch.zeros((1, n_frame, resize_image, resize_image), dtype=torch.float32)\n",
    "        state, info = env.reset(seed=random.randint(0, 2**24))\n",
    "        state = to_resize_gray(state, resize_image)\n",
    "        state_frame[:,0,:,:] = state\n",
    "        next_state_frame[:,0,:,:] = state\n",
    "        old_life = info['lives']\n",
    "    \n",
    "    for t in count():\n",
    "        total_steps +=1\n",
    "        action = e_greedy_select_action(state_frame, total_steps)\n",
    "        #action = boltzman_select_action(state_frame, i_episode)\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        \n",
    "        if old_life > info['lives']: # ライフが減った場合にtruncatedをTrueにして次エピソードにする\n",
    "            old_life = info['lives']\n",
    "            truncated = True\n",
    "        \n",
    "        if terminated or truncated: # ライフが減った場合にマイナスの報酬を与える\n",
    "            reward = -1 \n",
    "            \n",
    "        reward = torch.tensor([reward])\n",
    "\n",
    "        reward = torch.clamp(input=reward, min=-1, max=1)\n",
    "        \n",
    "        next_state = to_resize_gray(observation, resize_image)\n",
    "        # rollして一番古いフレームを新しいフレームで上書きする\n",
    "        next_state_frame = torch.roll(input = next_state_frame , shifts=1, dims=1)\n",
    "        next_state_frame[:,0,:,:] = next_state\n",
    "        \n",
    "        if frame1 == True:\n",
    "            state_frame1 = state_frame\n",
    "            action_frame1 = action\n",
    "            next_state_frame1 = next_state_frame\n",
    "            if terminated or truncated:\n",
    "                next_state_frame1 = None\n",
    "            frame1 = False\n",
    "            \n",
    "        reward_frame += reward\n",
    "            \n",
    "        if (total_steps % n_frame == 0) or terminated or truncated: \n",
    "            memory.push(state_frame1, action_frame1, next_state_frame1, reward_frame)\n",
    "            frame1 = True    \n",
    "            reward_frame = torch.tensor([0], dtype=torch.float32)\n",
    "                 \n",
    "            count_update += 1\n",
    "            if count_update % 4 == 0:\n",
    "                if count_update > size_replay_memory:\n",
    "                    optimize_model()\n",
    "            if count_update % 400 == 0:\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)   \n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "                \n",
    "        state_frame = next_state_frame\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "           \n",
    "    if (i_episode % num_episode_save) == 0:\n",
    "        torch.save(target_net.state_dict(), str(i_episode)+'.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
